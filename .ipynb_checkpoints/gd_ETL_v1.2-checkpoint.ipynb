{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "becoming-coalition",
   "metadata": {},
   "source": [
    "# <font color='red'>1.0 IMPORT</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "collectible-company",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-01T19:03:02.134271Z",
     "start_time": "2021-04-01T19:03:01.643826Z"
    }
   },
   "outputs": [],
   "source": [
    "# ENVIRONMENT\n",
    "\n",
    "# !pip install mysql-connector-python\n",
    "# !pip install pymysql\n",
    "# !pip install sqlalchemy\n",
    "\n",
    "import pandas as pd\n",
    "from datetime import datetime\n",
    "import mysql.connector\n",
    "import numpy as np\n",
    "from sqlalchemy import create_engine\n",
    "\n",
    "# pd.options.display.max_rows = 2000\n",
    "# pd.options.display.width = 120\n",
    "# pd.options.display.max_colwidth = 100\n",
    "\n",
    "ETL_VERSAO = 'etl_v1.0'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "abroad-collar",
   "metadata": {},
   "source": [
    "# <font color='red'>2.0 EXTRACT</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "blocked-hanging",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-01T19:03:02.142833Z",
     "start_time": "2021-04-01T19:03:02.136439Z"
    }
   },
   "outputs": [],
   "source": [
    "def LeFontes(strAut):\n",
    "#     sql_form = (\n",
    "#     \"SELECT `atributo`, `valor`, `protocolo` \"\n",
    "#     \"FROM form \"\n",
    "#     \"WHERE `atributo` IN ('state', 'city', 'neighborhood', 'zipcode', 'street')\"\n",
    "#     )\n",
    "    sql_form = (\n",
    "    \"SELECT * \"\n",
    "    \"FROM form\"\n",
    "    )\n",
    "    sql_tasks = (\n",
    "    \"SELECT `Protocolo`, `Entidade`, `Serviço`, `Usuário`, `Grupo`, `Data e Hora de conclusão`, \"\n",
    "    \"`Data e Hora de criação`, `Ação`, `Encaminhado para`, `Processo encerrado`, `Processo cancelado`, \"\n",
    "    \"`Motivo de cancelamento`, `Status externo`, `Categoria`, `Grupo responsável`, `Prazo (em segundos)` \"\n",
    "    \"FROM tasks\"\n",
    "    )\n",
    "    sql_sla = (\n",
    "    \"SELECT * \"\n",
    "    \"FROM sla\"\n",
    "    )\n",
    "    sql_rating = (\n",
    "    \"SELECT * \"\n",
    "    \"FROM rating\"\n",
    "    )\n",
    "    \n",
    "    try:\n",
    "        connection = mysql.connector.connect(host     = strAut['My_host'], \n",
    "                                             database = strAut['My_db'], \n",
    "                                             user     = strAut['My_user'], \n",
    "                                             password = strAut['My_pw'])\n",
    "\n",
    "        df_int_tasks =   pd.read_sql(sql_tasks,  con=connection)\n",
    "        df_int_sla =     pd.read_sql(sql_sla,    con=connection)\n",
    "        df_int_form =    pd.read_sql(sql_form,   con=connection)\n",
    "        df_int_rating =  pd.read_sql(sql_rating, con=connection)\n",
    "\n",
    "    except mysql.connector.Error as error:\n",
    "        print(\"Failed to read record from MySQL table {}\".format(error))\n",
    "\n",
    "    finally:\n",
    "        if (connection.is_connected()):\n",
    "            connection.close()\n",
    "            print(f'tasks:{df_int_tasks.shape[0]} registros lidos em {df_int_tasks.shape[1]} colunas')\n",
    "            print(f'sla: {df_int_sla.shape[0]} registros lidos em {df_int_sla.shape[1]} colunas')\n",
    "            print(f'form: {df_int_form.shape[0]} registros lidos em {df_int_form.shape[1]} colunas')\n",
    "            print(f'rating: {df_int_rating.shape[0]} registros lidos em {df_int_rating.shape[1]} colunas')\n",
    "            print(\"MySQL connection is closed\")\n",
    "            \n",
    "    return df_int_tasks, df_int_sla, df_int_form, df_int_rating"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "later-river",
   "metadata": {},
   "source": [
    "# <font color='red'>3.0 TRANSFORM</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "recovered-providence",
   "metadata": {},
   "source": [
    "## <font color='blue'>trata os datasets</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "alike-night",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "### <font color='black'>trata o dataset sla</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "weekly-martial",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-01T19:03:02.153444Z",
     "start_time": "2021-04-01T19:03:02.145480Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "def trSLA(df):\n",
    "    MINUTOS_PARA_DIAS = 1440 # quantidade de minutos em um dia\n",
    "\n",
    "    # preenche com null as colunas que contém 'null' como tipo string\n",
    "    df.loc[(df['limiteMinimo'] == 'null'), 'limiteMinimo'] = np.nan\n",
    "    df.loc[(df['limiteMaximo'] == 'null'), 'limiteMaximo'] = np.nan\n",
    "\n",
    "    # transforma o tipo de coluna para float\n",
    "    df['limiteMinimo'] = df['limiteMinimo'].astype(float)\n",
    "    df['limiteMaximo'] = df['limiteMaximo'].astype(float)\n",
    "\n",
    "    # transforma a unidade de medida de minuto para dias\n",
    "    df['limiteMinimo'] = df['limiteMinimo'] / MINUTOS_PARA_DIAS\n",
    "    df['limiteMaximo'] = df['limiteMaximo'] / MINUTOS_PARA_DIAS\n",
    "\n",
    "    # preenche com valores extremos os limites máximos e limites mínimos\n",
    "    df.loc[(df['limiteMinimo'].isna()), 'limiteMinimo'] = -9999999.9\n",
    "    df.loc[(df['limiteMaximo'].isna()), 'limiteMaximo'] = 9999999.9\n",
    "\n",
    "    # cria mais uma coluna de status para pivotar limite mínimo e limite máxio\n",
    "    df['status2'] = df['status']\n",
    "    df.rename(columns={'status': 'StatusLmin', 'status2': 'StatusLmax'}, inplace = True)\n",
    "    \n",
    "    # faz pivot da coluna StatusLmin\n",
    "    idx = ['entityCode', 'service', 'StatusLmax', 'limiteMaximo']\n",
    "    df = df.pivot(columns = 'StatusLmin', values = 'limiteMinimo', index=idx).reset_index()\n",
    "    df.columns.name = None\n",
    "    dic_renome = {'Dentro do prazo' : 'Dentro do Prazo LMin', \n",
    "                  'Fora do prazo' : 'Fora do Prazo LMin',\n",
    "                  'Perto do prazo' : 'Perto do Prazo LMin'}\n",
    "    df.rename(columns=dic_renome, inplace = True)\n",
    "\n",
    "    # faz pivot da coluna StatusLmax\n",
    "    idx = ['entityCode', 'service', 'Dentro do Prazo LMin', 'Fora do Prazo LMin', 'Perto do Prazo LMin']\n",
    "    df = df.pivot(columns = 'StatusLmax', values = 'limiteMaximo', index=idx).reset_index()\n",
    "    df.columns.name = None\n",
    "    dic_renome = {'Dentro do prazo' : 'Dentro do Prazo LMax', \n",
    "                  'Fora do prazo' : 'Fora do Prazo LMax',\n",
    "                  'Perto do prazo' : 'Perto do Prazo LMax'}\n",
    "    df.rename(columns=dic_renome, inplace = True)\n",
    "\n",
    "    # agrupa por entidade e serviço\n",
    "    df = df.groupby('service').sum().reset_index()\n",
    "\n",
    "    print('df_sla transformado')\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "adaptive-pierre",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "### <font color='black'>trata o dataset form</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "scheduled-auction",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-01T19:03:02.158707Z",
     "start_time": "2021-04-01T19:03:02.155714Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "def trForm(df):\n",
    "    \n",
    "    df.columns = ['atributo', 'nome', 'valor', 'protocolo', 'servico', 'tipo', 'campo_relacionado']\n",
    "    df['atributo'] = df['atributo'].str.lower()\n",
    "    \n",
    "    print('df_form transformado')\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "associate-yukon",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "### <font color='black'>trata o dataset rating</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "dynamic-jason",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-01T19:03:02.164868Z",
     "start_time": "2021-04-01T19:03:02.160898Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "def trRating(df):\n",
    "\n",
    "    # renomeia as colunas\n",
    "    cols = ['Solicitacao', 'NotaAvaliacao', 'MotivoAvaliacao', 'DataHoraAvaliacao']\n",
    "    df.columns = cols\n",
    "\n",
    "    # separa e formata as colunas de datas e horas \n",
    "    df['DataAvaliacao'] = pd.to_datetime(df['DataHoraAvaliacao']).dt.date\n",
    "    df['HoraAvaliacao'] = pd.to_datetime(df['DataHoraAvaliacao']).dt.time\n",
    "\n",
    "    # deleta a coluna que contém data e hora\n",
    "    df.drop(['DataHoraAvaliacao'], axis=1, inplace=True)\n",
    "\n",
    "    # preenche colunas de linhas vazias\n",
    "    df.loc[df['MotivoAvaliacao'] == '', 'MotivoAvaliacao'] = '<motivo vazio>'\n",
    "    \n",
    "    print('df_rating transformado')\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "several-democrat",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "### <font color='black'>trata o dataset tasks</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "assumed-blast",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-01T19:03:02.173129Z",
     "start_time": "2021-04-01T19:03:02.167538Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "def trTasks(df):\n",
    "\n",
    "    SEM_STATUS = '<sem status inicial>'\n",
    "    \n",
    "    # renomeia as colunas\n",
    "    lst_colunas_tasks = ['Protocolo', 'Entidade', 'Servico', 'Usuarios','Grupo', 'DataHora_Conclusao',\n",
    "                          'DataHora_Criacao', 'Acao', 'EncaminhadoPara', 'ProcessoEncerrado', 'ProcessoCancelado',\n",
    "                          'MotivoCancelamento', 'StatusExterno', 'Categoria', 'GrupoResponsavel','Prazo']\n",
    "    df.columns = lst_colunas_tasks\n",
    "    \n",
    "    # coloca dataset em ordem para acertar o status externo\n",
    "    lst = ['Entidade', 'Protocolo', 'DataHora_Criacao']\n",
    "    df = df.sort_values(by=lst).reset_index()\n",
    "\n",
    "    # rotina para preencher status externo vazio\n",
    "    # pega sempre o anterior e se o primeiro status estiver vazio coloca \"sem status inicial\"\n",
    "    if df.loc[0, 'StatusExterno'] == '':\n",
    "        df.loc[0, 'StatusExterno'] = SEM_STATUS\n",
    "    for i in range(1, df.shape[0]):\n",
    "        if df.loc[i, 'StatusExterno'] == '':    \n",
    "            if df.loc[i, 'Protocolo'] == df.loc[i - 1, 'Protocolo']:\n",
    "                df.loc[i, 'StatusExterno'] = df.loc[i - 1, 'StatusExterno']\n",
    "            else:\n",
    "                df.loc[i, 'StatusExterno'] = SEM_STATUS\n",
    "    \n",
    "    print('df_tasks transformado')\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "deadly-calgary",
   "metadata": {},
   "source": [
    "## <font color='blue'>monta tabelas dimensões</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "sought-interim",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "### DIM acoes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "digital-library",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-01T19:03:02.180408Z",
     "start_time": "2021-04-01T19:03:02.175488Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "def Mt_dim_Acoes(df):\n",
    "\n",
    "    # trasnforma a coluna Acao em uma coluna do tipo \"category'\n",
    "    df['tmpAcao'] = df['Acao'].astype('category')\n",
    "\n",
    "    # cria uma nova coluna que será a coluna chave primária da dimensão Ação\n",
    "    df['FK_dim_Acoes'] = df['tmpAcao'].cat.codes.astype('int64') + 1\n",
    "\n",
    "    # tira a duplicidade\n",
    "    dfx = df.loc[:, ['FK_dim_Acoes', 'Acao']].drop_duplicates()\n",
    "    dfx.rename(columns={'FK_dim_Acao': 'PK_dim_Acao'}, inplace = True)\n",
    "    \n",
    "    # preenche colunas com linhas vazias\n",
    "    dfx.loc[dfx['Acao'] == '', ['Acao']] = '<sem ação determinada>'\n",
    "    \n",
    "    # exclui a coluna Acoes de df_tasks que será a futura tabela fato\n",
    "    df.drop(['Acao', 'tmpAcao'], axis=1, inplace=True)\n",
    "    \n",
    "    print('dim_Acoes montada')\n",
    "\n",
    "    return dfx"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "considered-juice",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "### DIM categoria servicos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "medium-teaching",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-01T19:03:02.190646Z",
     "start_time": "2021-04-01T19:03:02.186202Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "def Mt_dim_CategoriaServico(df):\n",
    "\n",
    "    # trasnforma a coluna Categoria em uma coluna do tipo \"category'\n",
    "    df['tmpCategoria'] = df['Categoria'].astype('category')\n",
    "\n",
    "    # cria uma nova coluna que será a coluna chave primária da dimensão CategoriasServicos\n",
    "    df['FK_dim_CategoriasServicos'] = df['tmpCategoria'].cat.codes.astype('int64') + 1\n",
    "\n",
    "    # tira a duplicidade\n",
    "    dfx = df.loc[:, ['FK_dim_CategoriasServicos', 'Categoria']].drop_duplicates()\n",
    "    dfx.rename(columns={'FK_dim_CategoriasServicos': 'PK_dim_CategoriasServicos'}, inplace = True)\n",
    "\n",
    "    # preenche colunas com linhas vazias\n",
    "    dfx.loc[dfx['Categoria'] == '', ['Categoria']] = '<categoria indefinida>'\n",
    "    \n",
    "    # exclui a coluna Categoria de df_tasks que será a futura tabela fato\n",
    "    df.drop(['Categoria', 'tmpCategoria'], axis=1, inplace=True)\n",
    "    \n",
    "    print('dim_Categoria montada')\n",
    "\n",
    "    return dfx"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "manual-things",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "### DIM encaminhamento"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "comic-cookbook",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-01T19:03:02.199108Z",
     "start_time": "2021-04-01T19:03:02.194615Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "def Mt_dim_Encaminhamento(df):\n",
    "\n",
    "    # trasnforma a coluna EncaminhadoPara em uma coluna do tipo \"category'\n",
    "    df['tmpEncaminhadoPara'] = df['EncaminhadoPara'].astype('category')\n",
    "\n",
    "    # cria uma nova coluna que será a coluna chave primária da dimensão Encaminhamento\n",
    "    df['FK_dim_Encaminhamento'] = df['tmpEncaminhadoPara'].cat.codes.astype('int64') + 1\n",
    "\n",
    "    # tira a duplicidade\n",
    "    dfx = df.loc[:, ['FK_dim_Encaminhamento', 'EncaminhadoPara']].drop_duplicates()\n",
    "    dfx.rename(columns={'FK_dim_Encaminhamento': 'PK_dim_Encaminhamento'}, inplace = True)\n",
    "\n",
    "    # preenche colunas com linhas vazias\n",
    "    dfx.loc[dfx['EncaminhadoPara'] == '', ['EncaminhadoPara']] = '<sem encaminhamento>'\n",
    "\n",
    "    # exclui a coluna Entidade de df_tasks que será a futura tabela fato\n",
    "    df.drop(['EncaminhadoPara', 'tmpEncaminhadoPara'], axis=1, inplace=True)\n",
    "\n",
    "    print('dim_Encaminhamento montada')\n",
    "\n",
    "    return dfx"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "promising-portland",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "### DIM entidade"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "extraordinary-hungary",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-01T19:03:02.206572Z",
     "start_time": "2021-04-01T19:03:02.201526Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "def Mt_dim_Entidade(df):\n",
    "\n",
    "\n",
    "    # trasnforma a coluna Entidade em uma coluna do tipo \"category'\n",
    "    df['tmpEntidade'] = df['Entidade'].astype('category')\n",
    "\n",
    "    # cria uma nova coluna que será a coluna chave primária da dimensão Entidades\n",
    "    df['FK_dim_Entidades'] = df['tmpEntidade'].cat.codes.astype('int64') + 1\n",
    "\n",
    "    # tira a duplicidade\n",
    "    dfx = df.loc[:, ['FK_dim_Entidades', 'Entidade']].drop_duplicates()\n",
    "    dfx['Entidade'] = dfx['Entidade'].str.upper()\n",
    "    dfx.rename(columns={'FK_dim_Entidades': 'PK_dim_Entidades'}, inplace = True)\n",
    "\n",
    "    # exclui a coluna Entidade de df_tasks que será a futura tabela fato\n",
    "    df.drop(['Entidade', 'tmpEntidade'], axis=1, inplace=True)\n",
    "\n",
    "    print('dim_Entidade montada')\n",
    "\n",
    "    return dfx"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "working-silicon",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "### DIM grupo responsavel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "central-camera",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-01T19:03:02.213536Z",
     "start_time": "2021-04-01T19:03:02.209159Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "def Mt_dim_GrupoResponsavel(df):\n",
    "    \n",
    "    # trasnforma a coluna GrupoResponsavel em uma coluna do tipo \"category'\n",
    "    df['tmpGrupoResponsavel'] = df['GrupoResponsavel'].astype('category')\n",
    "\n",
    "    # cria uma nova coluna que será a coluna chave primária da dimensão GrupoResponsavel\n",
    "    df['FK_dim_GrupoResponsavel'] = df['tmpGrupoResponsavel'].cat.codes.astype('int64') + 1\n",
    "\n",
    "    # tira a duplicidade\n",
    "    dfx = df.loc[:, ['FK_dim_GrupoResponsavel', 'GrupoResponsavel']].drop_duplicates()\n",
    "    dfx.rename(columns={'FK_dim_GrupoResponsavel': 'PK_dim_GrupoResponsavel'}, inplace = True)\n",
    "\n",
    "    # preenche colunas com linhas vazias\n",
    "    dfx.loc[dfx['GrupoResponsavel'] == '', ['GrupoResponsavel']] = '<grupo responsável não definido>'\n",
    "\n",
    "    # exclui a coluna GrupoResponsavel de df_tasks que será a futura tabela fato\n",
    "    df.drop(['GrupoResponsavel', 'tmpGrupoResponsavel'], axis=1, inplace=True)\n",
    "    \n",
    "    print('dim_GrupoResponsavel montada')\n",
    "    \n",
    "    return dfx"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "contemporary-sample",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-03-04T11:19:20.012078Z",
     "start_time": "2021-03-04T11:19:20.009504Z"
    },
    "heading_collapsed": true
   },
   "source": [
    "### DIM grupo usuarios"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "charged-france",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-01T19:03:02.220953Z",
     "start_time": "2021-04-01T19:03:02.215885Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "def Mt_dim_GruposUsuarios(df):\n",
    "\n",
    "    # trasnforma a coluna Grupo em uma coluna do tipo \"category'\n",
    "    df['tmpGrupo'] = df['Grupo'].astype('category')\n",
    "\n",
    "    # cria uma nova coluna que será a coluna chave primária da dimensão Grupo\n",
    "    df['FK_dim_GruposUsuarios'] = df['tmpGrupo'].cat.codes.astype('int64') + 1\n",
    "\n",
    "    # tira a duplicidade\n",
    "    dfx = df.loc[:, ['FK_dim_GruposUsuarios', 'Grupo']].drop_duplicates()\n",
    "    dfx.rename(columns={'FK_dim_GruposUsuarios': 'PK_dim_GruposUsuarios'}, inplace = True)\n",
    "\n",
    "    # preenche colunas com linhas vazias\n",
    "    dfx.loc[dfx['Grupo'] == '', ['Grupo']] = '<grupo usuário não definido>'\n",
    "\n",
    "    # exclui a coluna Grupo de df_tasks que será a futura tabela fato\n",
    "    df.drop(['Grupo', 'tmpGrupo'], axis=1, inplace=True)\n",
    "    \n",
    "    print('dim_GruposUsuarios montada')\n",
    "    \n",
    "    return dfx"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "neither-grenada",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "### DIM motivos cancelamento"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "greatest-champagne",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-01T19:03:02.228253Z",
     "start_time": "2021-04-01T19:03:02.223639Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "def Mt_MotivosCanc(df):\n",
    "\n",
    "\n",
    "    # trasnforma a coluna MotivoCancelamento em uma coluna do tipo \"category'\n",
    "    df['tmpMotivoCancelamento'] = df['MotivoCancelamento'].astype('category')\n",
    "\n",
    "    # cria uma nova coluna que será a coluna chave primária da dimensão MotivoCanc\n",
    "    df['FK_dim_MotivosCanc'] = df['tmpMotivoCancelamento'].cat.codes.astype('int64') + 1\n",
    "\n",
    "    # tira a duplicidade\n",
    "    dfx = df.loc[:, ['FK_dim_MotivosCanc', 'MotivoCancelamento']].drop_duplicates()\n",
    "    dfx.rename(columns={'FK_dim_MotivosCanc': 'PK_dim_MotivosCanc'}, inplace = True)\n",
    "\n",
    "\n",
    "    # preenche colunas com linhas vazias\n",
    "    dfx.loc[dfx['MotivoCancelamento'] == '', ['MotivoCancelamento']] = '<sem motivo de cancelamento>'\n",
    "\n",
    "    # exclui a coluna MotivoCancelamento de df_tasks que será a futura tabela fato\n",
    "    df.drop(['MotivoCancelamento', 'tmpMotivoCancelamento'], axis=1, inplace=True)\n",
    "\n",
    "    print('dim_MotivosCanc montada')\n",
    "    \n",
    "    return dfx"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "unavailable-barbados",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "### DIM servico"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "weekly-universal",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-01T19:03:02.236347Z",
     "start_time": "2021-04-01T19:03:02.230494Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "def Mt_Servico(df, dff_sla):\n",
    "\n",
    "    # trasnforma a coluna Servico em uma coluna do tipo \"category'\n",
    "    df['tmpServico'] = df['Servico'].astype('category')\n",
    "\n",
    "    # cria uma nova coluna que será a coluna chave primária da dimensão Servico\n",
    "    df['FK_dim_Servicos'] = df['tmpServico'].cat.codes.astype('int64') + 1\n",
    "\n",
    "    # tira a duplicidade\n",
    "    dfx = df.loc[:, ['FK_dim_Servicos', 'Servico']].drop_duplicates()\n",
    "\n",
    "    # exclui a coluna Servico de df_tasks que será a futura tabela fato\n",
    "    df.drop(['Servico', 'tmpServico'], axis=1, inplace=True)\n",
    "\n",
    "    # defive os nomes das colunas de SLA\n",
    "    cols = {'FK_dim_Servicos': 'PK_dim_Servicos',\n",
    "            'Dentro do Prazo LMin': 'sla_VD_Lmin',\n",
    "            'Perto do Prazo LMin': 'sla_AM_Lmin',\n",
    "            'Fora do Prazo LMin': 'sla_VM_Lmin',\n",
    "            'Dentro do Prazo LMax': 'sla_VD_Lmax',\n",
    "            'Perto do Prazo LMax': 'sla_AM_LMax',\n",
    "            'Fora do Prazo LMax': 'sla_VM_LMax',\n",
    "           }\n",
    "    # faz um merge da dimensão dim_Servicos com a tabela de SLAs\n",
    "    dfx = dfx.merge(dff_sla, left_on='Servico', right_on='service', how='left')\n",
    "    dfx = dfx.drop('service', axis=1)\n",
    "    dfx.rename(columns=cols, inplace = True)\n",
    "    \n",
    "    print('dim_Servico montada')\n",
    "    \n",
    "    return dfx"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "undefined-occurrence",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "### DIM status externo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "prompt-rebecca",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-01T19:03:02.244650Z",
     "start_time": "2021-04-01T19:03:02.239579Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "def Mt_StatusExt(df):\n",
    "\n",
    "    # trasnforma a coluna StatusExterno em uma coluna do tipo \"category'\n",
    "    df['tmpStatusExterno'] = df['StatusExterno'].astype('category')\n",
    "\n",
    "    # cria uma nova coluna que será a coluna chave primária da dimensão Grupo\n",
    "    df['FK_dim_StatusExt'] = df['tmpStatusExterno'].cat.codes.astype('int64') + 1\n",
    "\n",
    "    # tira a duplicidade\n",
    "    dfx = df.loc[:, ['FK_dim_StatusExt', 'StatusExterno']].drop_duplicates()\n",
    "    dfx.rename(columns={'FK_dim_StatusExt': 'PK_dim_StatusExt'}, inplace = True)\n",
    "\n",
    "    # exclui a coluna Grupo de df_tasks que será a futura tabela fato\n",
    "    df.drop(['StatusExterno', 'tmpStatusExterno'], axis=1, inplace=True)\n",
    "\n",
    "    # preenche colunas com linhas vazias\n",
    "    dfx.loc[dfx['StatusExterno'] == '', ['StatusExterno']] = '<sem status>'\n",
    "\n",
    "    print('dim_StatusExt montada')\n",
    "    \n",
    "    return dfx"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "arbitrary-textbook",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "### DIM usuario"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "connected-theology",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-01T19:03:02.251989Z",
     "start_time": "2021-04-01T19:03:02.246976Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "def Mt_Usuarios(df):\n",
    "\n",
    "\n",
    "    # trasnforma a coluna Usuario em uma coluna do tipo \"category'\n",
    "    df['tmpUsuario'] = df['Usuarios'].astype('category')\n",
    "\n",
    "    # cria uma nova coluna que será a coluna chave primária da dimensão Usuario\n",
    "    df['FK_dim_Usuarios'] = df['tmpUsuario'].cat.codes.astype('int64') + 1\n",
    "\n",
    "    # tira a duplicidade\n",
    "    dfx = df.loc[:, ['FK_dim_Usuarios', 'Usuarios']].drop_duplicates()\n",
    "    col_ren = {'FK_dim_Usuarios': 'PK_dim_Usuarios', 'Usuarios': 'Usuario'}\n",
    "    dfx.rename(columns=col_ren, inplace = True)\n",
    "\n",
    "    # exclui a coluna Usuario de df_tasks que será a futura tabela fato\n",
    "    df.drop(['Usuarios', 'tmpUsuario'], axis=1, inplace=True)\n",
    "\n",
    "    # preenche colunas com linhas vazias\n",
    "    dfx.loc[dfx['Usuario'] == '', ['Usuario']] = '<usuário indefinido>'\n",
    "\n",
    "    print('dim_Usuario montada')\n",
    "    \n",
    "    return dfx"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "creative-string",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "### DIM situacao"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "previous-third",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-01T19:03:02.257710Z",
     "start_time": "2021-04-01T19:03:02.254692Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "def Mt_Situacao():\n",
    "\n",
    "    dfx = pd.DataFrame({'PK_dim_Situacao': [1, 2, 3],\n",
    "                        'Situacao':        ['em Andamento', 'Encerrada', 'Cancelada']}\n",
    "                        )\n",
    "    print('dim_Situacao montada')\n",
    "\n",
    "    return dfx\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "greek-click",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "### DIM SLA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "reported-starter",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-01T19:03:02.263997Z",
     "start_time": "2021-04-01T19:03:02.260483Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "def Mt_SLA():\n",
    "\n",
    "    dfx = pd.DataFrame({'PK_dim_SLA': [1, 2, 3],\n",
    "                        'DescSLA':    ['dentro do prazo', 'perto do prazo', 'fora do prazo'],\n",
    "                        'Cor':        ['verde','amarelo' ,'vermelho']}\n",
    "                    )\n",
    "    print('dim_SLA montada')\n",
    "\n",
    "    return dfx"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "universal-syndicate",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "### DIM endereco"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "broke-degree",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-01T19:03:02.275965Z",
     "start_time": "2021-04-01T19:03:02.266520Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "def Mt_Endereco(dfe):\n",
    "\n",
    "    dc = ['state', 'city', 'neighborhood', 'zipcode', 'street']\n",
    "    df = dfe.loc[dfe['atributo'].isin(dc), ['atributo','valor', 'protocolo']].copy()\n",
    "# ===================================================    \n",
    "    # faz pivot da coluna atributo\n",
    "    df = df.pivot(columns='atributo', values='valor', index='protocolo').reset_index()\n",
    "    df.columns.name = None\n",
    "\n",
    "    # renomeia colunas\n",
    "    dic_renome = {'protocolo': 'Solicitacao', \n",
    "                  'zipcode': 'CEP', \n",
    "                  'street': 'Endereco', \n",
    "                  'neighborhood': 'Bairro', \n",
    "                  'city': 'Cidade', \n",
    "                  'state': 'UF'}\n",
    "    df.rename(columns=dic_renome, inplace = True)\n",
    "    \n",
    "    # substitui a coluna Endereco por branco quando '-' ou null\n",
    "    df.loc[(df['Endereco'] == '-') | (df['Endereco'].isnull()), ['Endereco']] = ''\n",
    "    df.loc[(df['Bairro'] == '-') | (df['Bairro'].isnull()), ['Bairro']] = ''\n",
    "\n",
    "    # cria a coluna Endereco Completo\n",
    "    df['EnderecoCompleto'] = df['Cidade'] + ', ' + df['UF'] + ', Brasil' \n",
    "\n",
    "# ===================================================    \n",
    "\n",
    "    dict_uf = {'AC': 'Acre', 'AL': 'Alagoas', 'AP': 'Amapá', 'AM': 'Amazonas', 'BA': 'Bahia',\n",
    "               'CE': 'Ceará', 'ES': 'Espírito Santo', 'GO': 'Goiás', 'MA': 'Maranhão', 'MT': 'Mato Grosso',\n",
    "               'MS': 'Mato Grosso do Sul', 'MG': 'Minas Gerais', 'PA': 'Pará', 'PB': 'Paraíba',\n",
    "               'PR': 'Paraná', 'PE': 'Pernambuco', 'PI': 'Piauí', 'RJ': 'Rio de Janeiro',\n",
    "               'RN': 'Rio Grande do Norte', 'RS': 'Rio Grande do Sul', 'RO': 'Rondônia', 'RR': 'Roraima',\n",
    "               'SC': 'Santa Catarina', 'SP': 'São Paulo', 'SE': 'Sergipe', 'TO': 'Tocantins', 'DF': 'Distrito Federal',\n",
    "               '<nd>': '<sem UF>'\n",
    "    }\n",
    "\n",
    "    # acrescenta o nome do estado ao dataframe\n",
    "    dfx = df.copy()\n",
    "    dfx['NomeUF'] = dfx['UF'].apply(lambda x: dict_uf.get(x, None))\n",
    "    dfx.rename(columns={'Solicitacao': 'PK_dim_Endereco'}, inplace = True)\n",
    "\n",
    "    # preenche colunas com linhas vazias\n",
    "    dfx.loc[dfx['CEP'].isna(), ['CEP']] = ''\n",
    "\n",
    "    print('dim_Endereco montada')\n",
    "\n",
    "    return dfx"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "automatic-coordination",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "### FAT tasks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "violent-barbados",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-01T19:03:02.281678Z",
     "start_time": "2021-04-01T19:03:02.278381Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# função retorna a situação da solicitação de acordo com as coilunas cancelado e encerrado\n",
    "def RetSit(Enc, Canc):\n",
    "    rt = 0\n",
    "    if Canc == 1:\n",
    "        rt = 3\n",
    "    elif Enc == 1:\n",
    "        rt = 2\n",
    "    else:\n",
    "        rt = 1\n",
    "    return rt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "racial-religion",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-01T19:03:02.295069Z",
     "start_time": "2021-04-01T19:03:02.284247Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "def Mt_Tasks(df, dfr):\n",
    "\n",
    "    dfx = df.copy()\n",
    "    \n",
    "    # faz um merge com o dataset de ratings\n",
    "    dfx = df.merge(dfr, left_on='Protocolo', right_on='Solicitacao', how='left')\n",
    "    dfx.drop(['Solicitacao'], axis=1, inplace=True)\n",
    "\n",
    "    # define o código de situação da solicitação\n",
    "    df_aux = dfx[['Protocolo', 'ProcessoEncerrado', 'ProcessoCancelado']].drop_duplicates()\n",
    "\n",
    "    # substui valores de colunas\n",
    "    troca = {'false': 0, 'true': 1}\n",
    "    df_aux['ProcessoEncerrado'] = df_aux['ProcessoEncerrado'].map(troca)\n",
    "    df_aux['ProcessoCancelado'] = df_aux['ProcessoCancelado'].map(troca)\n",
    "\n",
    "    # agrega as ações para um registro por protocolo\n",
    "    df_aux = df_aux.groupby('Protocolo').agg({'ProcessoEncerrado': 'max', 'ProcessoCancelado': 'max'}).reset_index()\n",
    "\n",
    "    # retorna a situção da solicitação: 1-em andamento, 2-encerrado, 3-cancelado\n",
    "    df_aux['Situacao'] = df_aux.apply(lambda x: RetSit(x['ProcessoEncerrado'], x['ProcessoCancelado']), axis=1)\n",
    "    df_aux.drop(['ProcessoEncerrado', 'ProcessoCancelado'], axis=1, inplace=True)\n",
    "\n",
    "    # faz merge com o dataset de situação da solicitação\n",
    "    dfx = dfx.merge(df_aux, left_on='Protocolo', right_on='Protocolo', how='left')\n",
    "    dfx.drop(['ProcessoEncerrado', 'ProcessoCancelado'], axis=1, inplace=True)\n",
    "\n",
    "    # coloca -1 para as solicitações que não possuem avaliação\n",
    "    dfx.loc[dfx['NotaAvaliacao'].isna(), 'NotaAvaliacao'] = -1\n",
    "    dfx['NotaAvaliacao'] = dfx['NotaAvaliacao'].astype('int32')\n",
    "\n",
    "    # renomeia as colunas\n",
    "    col_ren = {'Protocolo': 'FK_dim_Solicitacoes', 'Situacao': 'FK_dim_Situacao'}\n",
    "    dfx.rename(columns=col_ren, inplace = True)\n",
    "\n",
    "    # trata a coluna Prazo\n",
    "    dfx['Prazo'] == ''\n",
    "    dfx.loc[dfx['Prazo'] == '', 'Prazo'] = '0'\n",
    "    dfx['Prazo'] = dfx['Prazo'].astype('int64')\n",
    "\n",
    "    dfx['DataCriacao'] = pd.to_datetime(dfx['DataHora_Criacao']).dt.date\n",
    "    dfx['HoraCriacao'] = pd.to_datetime(dfx['DataHora_Criacao']).dt.time\n",
    "\n",
    "    dfx['DataConclusao'] = pd.to_datetime(dfx['DataHora_Conclusao']).dt.date\n",
    "    dfx['HoraConclusao'] = pd.to_datetime(dfx['DataHora_Conclusao']).dt.time\n",
    "\n",
    "    dfx.drop(['DataHora_Criacao', 'DataHora_Conclusao'], axis=1, inplace=True)\n",
    "\n",
    "    cols_ordem = ['FK_dim_Solicitacoes', 'Prazo', 'DataCriacao', 'HoraCriacao', \n",
    "                  'DataConclusao', 'HoraConclusao', \n",
    "                  'NotaAvaliacao', 'MotivoAvaliacao', \n",
    "                  'DataAvaliacao', 'HoraAvaliacao', \n",
    "                  'FK_dim_Entidades', 'FK_dim_Servicos', 'FK_dim_Usuarios', 'FK_dim_GruposUsuarios', \n",
    "                  'FK_dim_Acoes', 'FK_dim_StatusExt', 'FK_dim_CategoriasServicos', 'FK_dim_GrupoResponsavel', \n",
    "                  'FK_dim_MotivosCanc', 'FK_dim_Encaminhamento', 'FK_dim_Situacao'\n",
    "                ]\n",
    "    dfx = dfx[cols_ordem]\n",
    "\n",
    "    print('fat_Tasks montada')\n",
    "\n",
    "    return dfx\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "medium-sandwich",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "### <font color='green'>META BI - Constantes</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "complex-gregory",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-01T19:03:02.303117Z",
     "start_time": "2021-04-01T19:03:02.297389Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# tabela de decisão de combinação Dimensões e Métricas\n",
    "mtx =  [('STRING_DIM',  'INTEGER_MET', 'N', 'N', 'S', 'S'),\n",
    "        ('STRING_DIM',  'DOUBLE_MET',  'N', 'N', 'S', 'S'),\n",
    "        ('STRING_DIM',  'NULL_MET',    'S', 'S', 'N', 'N'),\n",
    "\n",
    "        ('BOOLEAN_DIM', 'INTEGER_MET', 'S', 'N', 'N', 'N'),\n",
    "        ('BOOLEAN_DIM', 'DOUBLE_MET',  'S', 'N', 'N', 'N'),\n",
    "        ('BOOLEAN_DIM', 'NULL_MET',    'S', 'N', 'N', 'N'),\n",
    "\n",
    "        ('INTEGER_DIM', 'INTEGER_MET', 'S', 'S', 'S', 'S'),\n",
    "        ('INTEGER_DIM', 'DOUBLE_MET',  'N', 'N', 'N', 'N'),\n",
    "        ('INTEGER_DIM', 'NULL_MET',    'S', 'S', 'N', 'N'),\n",
    "\n",
    "        ('DOUBLE_MET',  'INTEGER_MET', 'N', 'N', 'N', 'N'),\n",
    "        ('DOUBLE_MET',  'DOUBLE_MET',  'N', 'N', 'S', 'S'),\n",
    "        ('DOUBLE_MET',  'NULL_MET',    'N', 'N', 'N', 'N'),\n",
    "\n",
    "        ('ID_DIM',      'INTEGER_MET', 'N', 'N', 'S', 'S'),\n",
    "        ('ID_DIM',      'DOUBLE_MET',  'N', 'N', 'S', 'S'),\n",
    "        ('ID_DIM',      'NULL_MET',    'S', 'S', 'N', 'N')\n",
    "        ]\n",
    "# colunas da tabela de decisão de combinação Dimensões e Métricas\n",
    "cols_mtx = ['t_dim', 't_met', 'Count', 'Distinct Count', 'Sum', 'Average']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "commercial-scanner",
   "metadata": {},
   "source": [
    "### <font color='green'>META BI - Tabelas Auxiliares</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "ordinary-progress",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-01T19:03:02.314249Z",
     "start_time": "2021-04-01T19:03:02.310693Z"
    }
   },
   "outputs": [],
   "source": [
    "def GeraCodDimMet(d, m):\n",
    "    d_dim = {'STRING_DIM': 10, 'BOOLEAN_DIM': 20, 'INTEGER_DIM' :30, 'DOUBLE_DIM': 40, 'ID_DIM': 50}\n",
    "    d_met = {'INTEGER_MET': 1, 'DOUBLE_MET': 2, 'NULL_MET': 3}\n",
    "\n",
    "    v_dim = d_dim[d] if d in d_dim else 0\n",
    "    v_met = d_met[m] if m in d_met else 0\n",
    "    \n",
    "    return v_dim + v_met"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "fatty-batman",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-01T19:03:02.321841Z",
     "start_time": "2021-04-01T19:03:02.317991Z"
    }
   },
   "outputs": [],
   "source": [
    "def Enumerico(s, tipo):\n",
    "    try:\n",
    "        v = np.float64(s) if tipo == 'DOUBLE' else np.int64(s)\n",
    "    except:\n",
    "        return False\n",
    "    \n",
    "    return True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "collaborative-mambo",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-01T19:03:02.327355Z",
     "start_time": "2021-04-01T19:03:02.324322Z"
    }
   },
   "outputs": [],
   "source": [
    "def Z_aux_entidade(df):\n",
    "    # recebe df=tasks\n",
    "    \n",
    "    df_ret = df[['Protocolo', 'Entidade']].drop_duplicates()\n",
    "    df_ret.columns = ['protocolo', 'entidade']\n",
    "    print('aux_entidade montada')\n",
    "\n",
    "    return df_ret"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "regulated-gates",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-01T19:03:02.334323Z",
     "start_time": "2021-04-01T19:03:02.329463Z"
    }
   },
   "outputs": [],
   "source": [
    "def Z_aux_BASE(df, df2):\n",
    "    # recebe df=form, df2=df_aux_entidade\n",
    "    \n",
    "    df_ret = df.copy()\n",
    "    \n",
    "    # transforma as strings atributo e campo relacionado em lower case\n",
    "    df_ret[['atributo', 'campo_relacionado']] = df_ret[['atributo', 'campo_relacionado']].apply(lambda x: x.str.lower())\n",
    "\n",
    "    # faz um merge da dimensão df_aux_BASE com a df_auxentidade para adicionar a coluna entidade\n",
    "    # se não existir o protocolo na tasks não considera a linha -- isso é um erro de integridade da base de dados\n",
    "    df_ret = df_ret.merge(df2, on='protocolo')    \n",
    "    \n",
    "    # cria 3 novas colunas vazias\n",
    "    df_ret['MET_atributo'] = np.nan\n",
    "    df_ret['MET_valor']    = np.nan\n",
    "    df_ret['MET_tipo']     = np.nan\n",
    "\n",
    "    # cria uma coluna como PK\n",
    "    df_ret['PK'] = df_ret['entidade'] + '_' + df_ret['servico'] + '_' + \\\n",
    "                        df_ret['atributo'] + '_' + df_ret['protocolo']\n",
    "\n",
    "    # cria uma coluna com o tipo de registro\n",
    "    df_ret['TIPO_REG'] = 'BASE'\n",
    "    \n",
    "    print('aux_BASE montada')\n",
    "    \n",
    "    return df_ret\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "limiting-victoria",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-01T19:03:02.344322Z",
     "start_time": "2021-04-01T19:03:02.336761Z"
    }
   },
   "outputs": [],
   "source": [
    "# traramento do tipo INTEGER ou DOUBLE\n",
    "def Z_aux_MET(df):\n",
    "    # recebe df=df_aux_BASE\n",
    "\n",
    "    # dataset de métricas - usa somente tipo \"INTEGER\" ou \"DOUBLE\"\n",
    "    cols = ['atributo', 'nome', 'valor', 'protocolo', 'servico', 'tipo', 'campo_relacionado', 'entidade']\n",
    "    df_ret = df.loc[(df['tipo'] == 'INTEGER') | (df['tipo'] == 'DOUBLE'), cols].copy()\n",
    "\n",
    "    # os valores estão com formato: ponto para separador de milhar e vírgula para fração\n",
    "    # tira os pontos separador de milhar e substitui vírgula por ponto\n",
    "    df_ret['valor'] = df_ret['valor'].apply(lambda x: x.replace('.', '').replace(',', '.'))\n",
    "\n",
    "    # alimenta uma coluna indicadora que mostra se o valor corresponde ou não ao tipo\n",
    "    df_ret['numerico'] = df_ret.apply(lambda x: Enumerico(x['valor'], x['tipo']), axis=1)\n",
    "\n",
    "    # cria um dataset de linhas cuja o valor não corresponde ao tipo\n",
    "    df_ret_Err = df_ret[df_ret['numerico'] == False].drop('numerico', axis=1)\n",
    "\n",
    "    # cria um dataset de linhas de métricas\n",
    "    df_ret = df_ret[~df_ret['numerico'] == False].drop('numerico', axis=1)\n",
    "\n",
    "    # identifica que esse é um dataset de métricas\n",
    "    df_ret['TIPO_REG'] = 'MET'\n",
    "\n",
    "    # exclui linhas que não seja uma métrica explícita\n",
    "    df_ret = df_ret[~(df_ret['campo_relacionado'] == 'null')]\n",
    "\n",
    "    # cria a coluna FK para relacionar com o dataset BASE\n",
    "    df_ret['FK'] = df_ret['entidade'] + '_' + df_ret['servico'] + '_' + \\\n",
    "                   df_ret['campo_relacionado'] + '_' + df_ret['protocolo']\n",
    "\n",
    "    # troca o nome de algumas colunas\n",
    "    dc = {'atributo': 'MET_atributo', 'campo_relacionado': 'atributo', 'valor': 'MET_valor', 'tipo': 'MET_tipo'}\n",
    "    df_ret.rename(columns=dc, inplace = True)\n",
    "\n",
    "    # faz um merge com o dataset BASE \n",
    "    df_ret = df_ret.merge(df[['PK', 'valor', 'tipo']], left_on='FK', right_on='PK')\n",
    "    df_ret.drop(['PK', 'FK'], axis=1, inplace=True)\n",
    "    \n",
    "    print('aux_MET e aux_MET_Err montada')\n",
    "\n",
    "    return df_ret, df_ret_Err\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "quarterly-brand",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-01T19:03:02.351208Z",
     "start_time": "2021-04-01T19:03:02.346918Z"
    }
   },
   "outputs": [],
   "source": [
    "def Z_aux_DIM(df, df_m):\n",
    "# recebe df=df_aux_BASE, df_m=df_aux_MET\n",
    "\n",
    "    # escolee as colunas que farão a cancatenação de dataframes\n",
    "    cols = ['atributo', 'nome', 'valor', 'protocolo', 'servico', 'tipo', \n",
    "            'entidade', 'MET_atributo', 'MET_valor', 'MET_tipo', 'TIPO_REG']\n",
    "    df_ret = df[cols].copy()\n",
    "    df_ret = pd.concat([df_ret, df_m])\n",
    "    \n",
    "    print('aux_DIM montada')\n",
    "\n",
    "    return df_ret"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "joint-feedback",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-01T19:03:02.358218Z",
     "start_time": "2021-04-01T19:03:02.354257Z"
    }
   },
   "outputs": [],
   "source": [
    "def Z_aux_decisao():\n",
    "    \n",
    "    df = pd.DataFrame(mtx, columns=cols_mtx)\n",
    "    df['Cod_Decisao'] = df.apply(lambda x: GeraCodDimMet(x['t_dim'], x['t_met']), axis=1)\n",
    "    \n",
    "    print('aux_decisao montada')\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "scientific-religious",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-01T19:03:02.364044Z",
     "start_time": "2021-04-01T19:03:02.360311Z"
    }
   },
   "outputs": [],
   "source": [
    "def Z_aux_DataSolicitacao(df):\n",
    "    # recebe df=tasks\n",
    "    \n",
    "    df_tmp = df[['Protocolo', 'DataHora_Criacao']].copy()\n",
    "    # cria uma coluna só de datas\n",
    "    df_tmp['data_criacao'] = pd.to_datetime(df_tmp['DataHora_Criacao']).dt.date\n",
    "\n",
    "    # pega a menor data de cada número de protocolo\n",
    "    df_ret = df_tmp[['Protocolo', 'data_criacao']].groupby('Protocolo').agg('min').reset_index()\n",
    "    df_ret.columns = ['protocolo', 'data_criacao']\n",
    "    \n",
    "    print('aux_DataSolicitacao montada')\n",
    "\n",
    "    return df_ret\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "available-electricity",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-01T19:03:02.371650Z",
     "start_time": "2021-04-01T19:03:02.366534Z"
    }
   },
   "outputs": [],
   "source": [
    "def Z_aux_pto(df):\n",
    "    # recebe df=df_aux_dec\n",
    "\n",
    "    # faz uma cópia da tabela de decisão com as colunas necessárias\n",
    "    cols = ['Cod_Decisao'] + cols_mtx[2:]\n",
    "    df_ret = df[cols].copy()\n",
    "\n",
    "    # faz o unpivot\n",
    "    df_ret = pd.melt(df_ret, id_vars=['Cod_Decisao'], \n",
    "                     value_vars=['Count', 'Distinct Count', 'Sum', 'Average'],\n",
    "                     var_name='Atributo')\n",
    "    df_ret = df_ret[df_ret['value'] == 'S'].drop('value', axis=1)\n",
    "\n",
    "    # trasnforma a coluna Atributo em uma coluna do tipo \"category'\n",
    "    df_ret['Atributo'] = df_ret['Atributo'].astype('category')\n",
    "\n",
    "    # cria uma nova coluna que será a coluna chave primária da dimensão Operação\n",
    "    df_ret['FK_ZN_dim_OPERACAO'] = df_ret['Atributo'].cat.codes.astype('int64') + 1\n",
    "    \n",
    "    print('aux_pto montada')\n",
    "\n",
    "    return df_ret"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "human-cambodia",
   "metadata": {},
   "source": [
    "### <font color='green'>META BI - Tabelas Dim, Fat e Pto</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "variable-frequency",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-01T19:03:02.377257Z",
     "start_time": "2021-04-01T19:03:02.373881Z"
    }
   },
   "outputs": [],
   "source": [
    "def Z_pto_Decisao_Operacao(df):\n",
    "    # recebe df=df_Z_aux_pto\n",
    "\n",
    "    df_ret = df.copy()\n",
    "    \n",
    "    # cria a tabela df_pto_decisao_operacao necessária ao modelo dimensional\n",
    "    df_ret = df_ret[['Cod_Decisao', 'FK_ZN_dim_OPERACAO']]\n",
    "    df_ret.columns = ['FK_dim_DECISAO', 'FK_ZN_dim_OPERACAO']\n",
    "    \n",
    "    print('pto_decisao_operacao montada')\n",
    "\n",
    "    return df_ret\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "ceramic-leather",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-01T19:03:02.383028Z",
     "start_time": "2021-04-01T19:03:02.379319Z"
    }
   },
   "outputs": [],
   "source": [
    "def Z_dim_DIM(df):\n",
    "    # recebe df=df_aux_DIM\n",
    "\n",
    "    df_ret = df.copy()\n",
    "\n",
    "    df_ret = df_ret[['atributo', 'nome']].drop_duplicates().reset_index(drop=True).reset_index()\n",
    "    df_ret.columns = ['PK_Z_dim_DIM', 'dimensao', 'nome']\n",
    "    df_ret['PK_Z_dim_DIM'] += 1\n",
    "    \n",
    "    print('dim_DIM montada')\n",
    "\n",
    "    return df_ret\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "pursuant-passion",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-01T19:03:02.389198Z",
     "start_time": "2021-04-01T19:03:02.385309Z"
    }
   },
   "outputs": [],
   "source": [
    "def Z_dim_SERVICO(df):    \n",
    "    # recebe df=df_aux_DIM\n",
    "\n",
    "    df_ret = df.copy()\n",
    "\n",
    "    df_ret = df_ret['servico'].drop_duplicates().reset_index(drop=True).reset_index()\n",
    "    df_ret.columns = ['PK_Z_dim_SERVICO', 'servico']\n",
    "    df_ret['PK_Z_dim_SERVICO'] += 1\n",
    "    \n",
    "    print('dim_SERVICO montada')\n",
    "\n",
    "    return df_ret\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "selective-arlington",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-01T19:03:02.395683Z",
     "start_time": "2021-04-01T19:03:02.391511Z"
    }
   },
   "outputs": [],
   "source": [
    "def Z_dim_MET(df):    \n",
    "    # recebe df=df_aux_DIM\n",
    "\n",
    "    df_ret = df.copy()\n",
    "\n",
    "    df_ret = df_ret.loc[~df_ret['MET_atributo'].isna(), 'MET_atributo'].drop_duplicates()\n",
    "    df_ret = df_ret.reset_index(drop=True).reset_index()\n",
    "    df_ret.columns = ['PK_Z_dim_MET', 'metrica']\n",
    "    df_ret['PK_Z_dim_MET'] += 1\n",
    "\n",
    "    print('dim_MET montada')\n",
    "\n",
    "    return df_ret\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "hungry-vegetable",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-01T19:03:02.400903Z",
     "start_time": "2021-04-01T19:03:02.397584Z"
    }
   },
   "outputs": [],
   "source": [
    "def Z_dim_OPERACAO(df):    \n",
    "    # recebe df=df_aux_pto\n",
    "\n",
    "    df_ret = df.copy()\n",
    "\n",
    "    df_ret = df_ret[['FK_ZN_dim_OPERACAO', 'Atributo']].drop_duplicates()\n",
    "    df_ret.columns = ['PK_Z_dim_OPERACAO', 'operacao']\n",
    "\n",
    "    print('dim_OPERACAO montada')\n",
    "\n",
    "    return df_ret\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "middle-custom",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-01T19:03:02.406888Z",
     "start_time": "2021-04-01T19:03:02.403163Z"
    }
   },
   "outputs": [],
   "source": [
    "def Z_dim_DECISAO(df):    \n",
    "    # recebe df=df_aux_dec\n",
    "\n",
    "    df_ret = df.copy()\n",
    "\n",
    "    df_ret = df_ret[['Cod_Decisao', 't_dim', 't_met']].copy().rename(columns={'Cod_Decisao': 'PK_Z_dim_DECISAO'})\n",
    "\n",
    "    print('dim_DECISAO montada')\n",
    "\n",
    "    return df_ret\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "tight-patent",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-01T19:03:02.418804Z",
     "start_time": "2021-04-01T19:03:02.408842Z"
    }
   },
   "outputs": [],
   "source": [
    "def Z_fat_META(df, df_dim, df_ser, df_met, df_dts):    \n",
    "    # recebe df=df_Z_dim_DIM, df_ser=df_Z_dim_SERVICO, df_met= df_Z_dim_MET, df_dts=df_Z_aux_DataSolicitacao\n",
    "\n",
    "    # copia o df auxiliar\n",
    "    df_ret = df.copy()\n",
    "\n",
    "    # faz merge com dim_DIM\n",
    "    df_ret = df_ret.merge(df_dim, \n",
    "                          how='left', \n",
    "                          left_on=['atributo', 'nome'], \n",
    "                          right_on=['dimensao', 'nome']).drop(['atributo', 'nome', 'dimensao'], axis=1)\n",
    "    # faz merge com dim_SERVICO\n",
    "    df_ret = df_ret.merge(df_ser, \n",
    "                          how='left', \n",
    "                          left_on='servico', \n",
    "                          right_on='servico').drop('servico', axis=1)\n",
    "    # faz merge com dim_MET\n",
    "    df_ret = df_ret.merge(df_met, \n",
    "                          how='left', \n",
    "                          left_on='MET_atributo', \n",
    "                          right_on='metrica').drop(['MET_atributo', 'metrica'], axis=1)\n",
    "    # faz merge com Z_aux_DataSolicitacao\n",
    "    df_ret = df_ret.merge(df_dts, \n",
    "                          how='left', \n",
    "                          on='protocolo') #.drop(['MET_atributo', 'metrica'], axis=1)\n",
    "\n",
    "    # cria a coluna indicadora de NULL_MET\n",
    "    df_ret['reg_nativo'] = df_ret['MET_tipo'].apply(lambda x: 1 if pd.isna(x) else 0)\n",
    "\n",
    "    # passa as colunas de tipo para a string indicadora de NULL\n",
    "    df_ret.loc[df_ret['MET_tipo'].isna(), 'MET_tipo'] = 'NULL_MET'\n",
    "    df_ret.loc[df_ret['tipo'].isna(), 'tipo'] = 'NULL_DIM'\n",
    "\n",
    "\n",
    "    # substitui valores da coluna tipo\n",
    "    subs_dim = {'STRING':'STRING_DIM', \n",
    "                'INTEGER': 'INTEGER_DIM', \n",
    "                'BOOLEAN':'BOOLEAN_DIM', \n",
    "                'DOUBLE': 'DOUBLE_DIM',\n",
    "                'ID':'ID_DIM'}\n",
    "    subs_met = {'INTEGER': 'INTEGER_MET', \n",
    "                'DOUBLE': 'DOUBLE_MET'}\n",
    "\n",
    "    df_ret['tipo'].replace(subs_dim, inplace=True)\n",
    "    df_ret['MET_tipo'].replace(subs_met, inplace=True)\n",
    "\n",
    "    # acha o código para a tabela de decisão da dupla de tipos como FK para a PK de \"df_aux_dec\"\n",
    "    df_ret['FK_Z_dim_DECISAO'] = df_ret.apply(lambda x: GeraCodDimMet(x['tipo'], x['MET_tipo']), axis=1)\n",
    "\n",
    "    # exclui colunas já usadas\n",
    "    df_ret.drop(['tipo', 'MET_tipo'], axis=1, inplace=True)\n",
    "\n",
    "    # acerta os nomes das colunas\n",
    "    dc = {'PK_Z_dim_DIM': 'FK_Z_dim_DIM', \n",
    "          'PK_Z_dim_SERVICO': 'FK_Z_dim_SERVICO', \n",
    "          'PK_Z_dim_MET': 'FK_Z_dim_MET', \n",
    "          'data_criacao': 'FK_Z_Data_Criacao'}\n",
    "    df_ret.rename(columns=dc, inplace = True)\n",
    "\n",
    "    # acerta os tipos das colunas\n",
    "    df_ret['FK_Z_dim_MET'] = df_ret['FK_Z_dim_MET'].astype('Int64')\n",
    "    df_ret['MET_valor'] = df_ret['MET_valor'].astype('float64')\n",
    "    df_ret['FK_Z_Data_Criacao'] = df_ret['FK_Z_Data_Criacao'].apply(pd.to_datetime)\n",
    "\n",
    "    print('fat_META montada')\n",
    "\n",
    "    return df_ret\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "patient-break",
   "metadata": {},
   "source": [
    "# <font color='red'>4.0 LOAD</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "handed-genius",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-01T19:03:02.434060Z",
     "start_time": "2021-04-01T19:03:02.421474Z"
    }
   },
   "outputs": [],
   "source": [
    "def Load_BD(strAut):\n",
    "\n",
    "    # import the module\n",
    "    # create sqlalchemy engine\n",
    "    \n",
    "    SQLengine = create_engine(\"mysql+pymysql://{user}:{pw}@{host}/{db}\"\n",
    "                           .format(user = strAut['My_user'],\n",
    "                                   pw   = strAut['My_pw'],\n",
    "                                   host = strAut['My_host'],\n",
    "                                   db   = strAut['My_db']),\n",
    "                                   pool_recycle=3600)\n",
    "    dbConn = SQLengine.connect()\n",
    "\n",
    "    try:\n",
    "        df_dim_Acao.to_sql(               'dim_acoes',               \n",
    "                                           con=dbConn, if_exists='replace', \n",
    "                                           index=False, chunksize = 1000)    \n",
    "        df_dim_Categoria.to_sql(          'dim_categorias_servicos', \n",
    "                                           con=dbConn, if_exists='replace', \n",
    "                                           index=False, chunksize = 1000)\n",
    "        df_dim_Encaminhamento.to_sql(     'dim_encaminhamento',      \n",
    "                                           con=dbConn, if_exists='replace', \n",
    "                                           index=False, chunksize = 1000)\n",
    "        df_dim_Entidade.to_sql(           'dim_entidades',           \n",
    "                                           con=dbConn, if_exists='replace', \n",
    "                                           index=False, chunksize = 1000)\n",
    "        df_dim_GrupoResponsavel.to_sql(   'dim_grupo_responsavel',   \n",
    "                                           con=dbConn, if_exists='replace', \n",
    "                                           index=False, chunksize = 1000)\n",
    "        df_dim_Grupo.to_sql(              'dim_grupos_usuarios',     \n",
    "                                           con=dbConn, if_exists='replace', \n",
    "                                           index=False, chunksize = 1000)\n",
    "        df_dim_MotivoCanc.to_sql(         'dim_motivos_canc',        \n",
    "                                           con=dbConn, if_exists='replace', \n",
    "                                           index=False, chunksize = 1000)\n",
    "        df_dim_Servicos.to_sql(           'dim_servicos',            \n",
    "                                           con=dbConn, if_exists='replace', \n",
    "                                           index=False, chunksize = 1000)\n",
    "        df_dim_StatusExt.to_sql(          'dim_status_ext',          \n",
    "                                           con=dbConn, if_exists='replace', \n",
    "                                           index=False, chunksize = 1000)\n",
    "        df_dim_Usuarios.to_sql(           'dim_usuarios',            \n",
    "                                           con=dbConn, if_exists='replace', \n",
    "                                           index=False, chunksize = 1000)\n",
    "        df_dim_Situacao.to_sql(           'dim_situacao',            \n",
    "                                           con=dbConn, if_exists='replace', \n",
    "                                           index=False, chunksize = 1000)\n",
    "        df_dim_SLA.to_sql(                'dim_sla',                 \n",
    "                                           con=dbConn, if_exists='replace', \n",
    "                                           index=False, chunksize = 1000)\n",
    "        df_dim_Endereco.to_sql(           'dim_endereco',            \n",
    "                                           con=dbConn, if_exists='replace', \n",
    "                                           index=False, chunksize = 1000)\n",
    "\n",
    "        df_fat_tasks.to_sql(              'fat_tasks',               \n",
    "                                           con=dbConn, if_exists='replace', \n",
    "                                           index=False, chunksize = 1000)\n",
    "\n",
    "        df_Z_pto_decisao_operacao.to_sql( 'z_pto_decisao_operacao',  \n",
    "                                           con=dbConn, if_exists='replace', \n",
    "                                           index=False, chunksize = 1000)\n",
    "        df_Z_dim_DIM.to_sql(              'z_dim_dim',  \n",
    "                                           con=dbConn, if_exists='replace', \n",
    "                                           index=False, chunksize = 1000)\n",
    "        df_Z_dim_SERVICO.to_sql(          'z_dim_servico',  \n",
    "                                           con=dbConn, if_exists='replace', \n",
    "                                           index=False, chunksize = 1000)\n",
    "        df_Z_dim_MET.to_sql(              'z_dim_met',  \n",
    "                                           con=dbConn, if_exists='replace', \n",
    "                                           index=False, chunksize = 1000)\n",
    "        df_Z_dim_OPERACAO.to_sql(         'z_dim_operacao',  \n",
    "                                           con=dbConn, if_exists='replace', \n",
    "                                           index=False, chunksize = 1000)\n",
    "        df_Z_dim_DECISAO.to_sql(          'z_dim_decisao',  \n",
    "                                           con=dbConn, if_exists='replace', \n",
    "                                           index=False, chunksize = 1000)\n",
    "        df_Z_fat_META.to_sql(             'z_fat_meta',  \n",
    "                                           con=dbConn, if_exists='replace', \n",
    "                                           index=False, chunksize = 1000)\n",
    "        df_ts.to_sql(                     'ctl_carga',  \n",
    "                                           con=dbConn, if_exists='append', \n",
    "                                           index=False, chunksize = 1000)\n",
    "\n",
    "\n",
    "    except ValueError as vx:\n",
    "        print('ERROR -', vx)\n",
    "\n",
    "    except Exception as ex: \n",
    "        print('EXCEPTION -', ex)\n",
    "\n",
    "    else:\n",
    "        print('Tabelas criadas com sucesso');  \n",
    "\n",
    "    finally:\n",
    "        dbConn.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "comic-asian",
   "metadata": {},
   "source": [
    "# <font color='red'>0.0 MAIN</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "contemporary-leone",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-01T19:03:02.440672Z",
     "start_time": "2021-04-01T19:03:02.436313Z"
    }
   },
   "outputs": [],
   "source": [
    "# monta as strings de conexão\n",
    "def StrConn(sit):\n",
    "    in_oper   = {'My_host': 'localhost', 'My_db': 'bd_fontes',       'My_user': 'gd', 'My_pw': 'Alpar@123'}\n",
    "    out_oper  = {'My_host': 'localhost', 'My_db': 'bd_dw',           'My_user': 'gd', 'My_pw': 'Alpar@123'}\n",
    "    in_teste  = {'My_host': 'localhost', 'My_db': 'bd_teste_fontes', 'My_user': 'gd', 'My_pw': 'Alpar@123'}\n",
    "    out_teste = {'My_host': 'localhost', 'My_db': 'bd_teste_dw',     'My_user': 'gd', 'My_pw': 'Alpar@123'}\n",
    "    \n",
    "    # se True é teste\n",
    "    if sit:\n",
    "        rt_in = in_teste\n",
    "        rt_out = out_teste\n",
    "    else:\n",
    "        rt_in = in_oper\n",
    "        rt_out = out_oper\n",
    "\n",
    "    return rt_in, rt_out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "combined-province",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-01T19:03:02.447107Z",
     "start_time": "2021-04-01T19:03:02.443028Z"
    }
   },
   "outputs": [],
   "source": [
    "def gd_timestamp():\n",
    "    lst = {'DataHora': [datetime.today()], 'Varsao': [ETL_VERSAO]}\n",
    "    df = pd.DataFrame(lst)\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "instructional-campbell",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-01T19:03:03.539306Z",
     "start_time": "2021-04-01T19:03:02.449308Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tasks:619 registros lidos em 16 colunas\n",
      "sla: 54 registros lidos em 5 colunas\n",
      "form: 2034 registros lidos em 7 colunas\n",
      "rating: 30 registros lidos em 4 colunas\n",
      "MySQL connection is closed\n",
      "df_sla transformado\n",
      "df_form transformado\n",
      "df_rating transformado\n",
      "df_tasks transformado\n",
      "dim_Acoes montada\n",
      "dim_Categoria montada\n",
      "dim_Encaminhamento montada\n",
      "dim_Entidade montada\n",
      "dim_GrupoResponsavel montada\n",
      "dim_GruposUsuarios montada\n",
      "dim_MotivosCanc montada\n",
      "dim_Servico montada\n",
      "dim_StatusExt montada\n",
      "dim_Usuario montada\n",
      "dim_Situacao montada\n",
      "dim_SLA montada\n",
      "dim_Endereco montada\n",
      "fat_Tasks montada\n",
      "aux_entidade montada\n",
      "aux_BASE montada\n",
      "aux_MET e aux_MET_Err montada\n",
      "aux_DIM montada\n",
      "aux_decisao montada\n",
      "aux_DataSolicitacao montada\n",
      "aux_pto montada\n",
      "pto_decisao_operacao montada\n",
      "dim_DIM montada\n",
      "dim_SERVICO montada\n",
      "dim_MET montada\n",
      "dim_OPERACAO montada\n",
      "dim_DECISAO montada\n",
      "fat_META montada\n",
      "Tabelas criadas com sucesso\n"
     ]
    }
   ],
   "source": [
    "if __name__ == '__main__':\n",
    "    \n",
    "    etl_teste = False\n",
    "\n",
    "    # EXTRACT\n",
    "    AUTENTIC_IN, AUTENTIC_OUT = StrConn(etl_teste)\n",
    "    df_tasks, df_sla, df_form, df_rating = LeFontes(AUTENTIC_IN)\n",
    "    \n",
    "    # TRANSFORM\n",
    "    # trata os datasets\n",
    "    df_sla    = trSLA(df_sla)\n",
    "    df_form   = trForm(df_form)\n",
    "    df_rating = trRating(df_rating)\n",
    "    df_tasks  = trTasks(df_tasks)\n",
    "    \n",
    "    dft = df_tasks.copy()\n",
    "    dff = df_form.copy()\n",
    "    \n",
    "    # monta as tabelas dimensão\n",
    "    df_dim_Acao              = Mt_dim_Acoes(df_tasks)\n",
    "    df_dim_Categoria         = Mt_dim_CategoriaServico(df_tasks)\n",
    "    df_dim_Encaminhamento    = Mt_dim_Encaminhamento(df_tasks)\n",
    "    df_dim_Entidade          = Mt_dim_Entidade(df_tasks)\n",
    "    df_dim_GrupoResponsavel  = Mt_dim_GrupoResponsavel(df_tasks)\n",
    "    df_dim_Grupo             = Mt_dim_GruposUsuarios(df_tasks)\n",
    "    df_dim_MotivoCanc        = Mt_MotivosCanc(df_tasks)\n",
    "    df_dim_Servicos          = Mt_Servico(df_tasks, df_sla)\n",
    "    df_dim_StatusExt         = Mt_StatusExt(df_tasks)\n",
    "    df_dim_Usuarios          = Mt_Usuarios(df_tasks)\n",
    "    df_dim_Situacao          = Mt_Situacao()\n",
    "    df_dim_SLA               = Mt_SLA()\n",
    "    df_dim_Endereco          = Mt_Endereco(df_form)\n",
    "    \n",
    "    # monta a tabela fato\n",
    "    df_fat_tasks             = Mt_Tasks(df_tasks, df_rating)\n",
    "    \n",
    "    # monta as tabelas auxiliares do META BI\n",
    "    df_Z_aux_entidade              = Z_aux_entidade(dft)\n",
    "    df_Z_aux_BASE                  = Z_aux_BASE(dff, df_Z_aux_entidade)\n",
    "    df_Z_aux_MET, df_Z_aux_MET_Err = Z_aux_MET(df_Z_aux_BASE)\n",
    "    df_Z_aux_DIM                   = Z_aux_DIM(df_Z_aux_BASE, df_Z_aux_MET)\n",
    "    df_Z_aux_decisao               = Z_aux_decisao()\n",
    "    df_Z_aux_DataSolicitacao       = Z_aux_DataSolicitacao(dft)\n",
    "    df_Z_aux_pto                   = Z_aux_pto(df_Z_aux_decisao)\n",
    "    \n",
    "    # monta as tabelas dimensão do META BI\n",
    "    df_Z_pto_decisao_operacao      = Z_pto_Decisao_Operacao(df_Z_aux_pto)\n",
    "    df_Z_dim_DIM                   = Z_dim_DIM(df_Z_aux_DIM)\n",
    "    df_Z_dim_SERVICO               = Z_dim_SERVICO(df_Z_aux_DIM)\n",
    "    df_Z_dim_MET                   = Z_dim_MET(df_Z_aux_DIM)\n",
    "    df_Z_dim_OPERACAO              = Z_dim_OPERACAO(df_Z_aux_pto)\n",
    "    df_Z_dim_DECISAO               = Z_dim_DECISAO(df_Z_aux_decisao)\n",
    "\n",
    "    # monta as tabelas dimensão do META BI\n",
    "    df_Z_fat_META                  = Z_fat_META(df_Z_aux_DIM,\n",
    "                                                df_Z_dim_DIM, \n",
    "                                                df_Z_dim_SERVICO, \n",
    "                                                df_Z_dim_MET, \n",
    "                                                df_Z_aux_DataSolicitacao)\n",
    "    \n",
    "    # monta data e hora e versão da atualização\n",
    "    df_ts = gd_timestamp()\n",
    "\n",
    "    # LOAD\n",
    "    ret = Load_BD(AUTENTIC_OUT)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "adapted-jonathan",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
