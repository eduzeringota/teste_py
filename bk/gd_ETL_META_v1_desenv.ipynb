{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <font color='blue'>ALPAR - Governo Digital - Processo Inteligente</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <font color='red'>ETL para o BI</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.0 import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-03-31T17:25:24.005787Z",
     "start_time": "2021-03-31T17:25:24.003157Z"
    }
   },
   "outputs": [],
   "source": [
    "# !pip install pandas\n",
    "# !pip install mysql-connector-python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-03-31T17:25:25.413765Z",
     "start_time": "2021-03-31T17:25:24.743268Z"
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import mysql.connector\n",
    "from mysql.connector import Error\n",
    "from mysql.connector import errorcode# import numpy as np\n",
    "# import copy\n",
    "# import csv\n",
    "# import codecs\n",
    "# import time\n",
    "# from os.path import expanduser\n",
    "\n",
    "\n",
    "\n",
    "# print(\"pandas versão\", pd.__version__)\n",
    "# print(\"numpy versão\", np.__version__)\n",
    "# print(\"csv versão\", csv.__version__)\n",
    "\n",
    "# pd.options.display.max_rows = 2000\n",
    "# pd.options.display.width = 120\n",
    "# pd.options.display.max_colwidth = 100"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <font color='black'>2.0 load datasets</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-03-31T17:25:29.049582Z",
     "start_time": "2021-03-31T17:25:29.047234Z"
    }
   },
   "outputs": [],
   "source": [
    "My_host = 'localhost'\n",
    "My_db = 'bd_fontes'\n",
    "My_user = 'gd'\n",
    "My_pw = 'Alpar@123'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-03-31T17:25:29.920057Z",
     "start_time": "2021-03-31T17:25:29.917231Z"
    }
   },
   "outputs": [],
   "source": [
    "sql_form = (\n",
    "\"SELECT * \"\n",
    "\"FROM form\"\n",
    ")\n",
    "sql_tasks = (\n",
    "\"SELECT \"\n",
    "\"Protocolo as 'protocolo', \"\n",
    "\"Entidade as 'entidade', \"\n",
    "\"`Data e Hora de criação` as 'data_hora_criacao' \"\n",
    "\"FROM tasks\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-03-31T17:25:31.485624Z",
     "start_time": "2021-03-31T17:25:31.328891Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2034 registros lidos em 7 colunas\n",
      "619 registros lidos em 3 colunas\n",
      "MySQL connection is closed\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    connection = mysql.connector.connect(host = My_host, database = My_db, user = My_user, password = My_pw)\n",
    "    \n",
    "    df_form = pd.read_sql(sql_form, con=connection)\n",
    "    df_tasks = pd.read_sql(sql_tasks, con=connection)\n",
    "    \n",
    "except mysql.connector.Error as error:\n",
    "    print(\"Failed to read record from MySQL table {}\".format(error))\n",
    "\n",
    "finally:\n",
    "    if (connection.is_connected()):\n",
    "        connection.close()\n",
    "        print(f'{df_form.shape[0]} registros lidos em {df_form.shape[1]} colunas')\n",
    "        print(f'{df_tasks.shape[0]} registros lidos em {df_tasks.shape[1]} colunas')\n",
    "\n",
    "        print(\"MySQL connection is closed\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <font color='black'>3.0 prepara tabelas auxiliares</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1 prepara tabela auxiliar tasks (Z_aux_entidade)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-03-31T17:25:35.065639Z",
     "start_time": "2021-03-31T17:25:35.060581Z"
    }
   },
   "outputs": [],
   "source": [
    "df_aux_entidade = df_tasks[['protocolo', 'entidade']].drop_duplicates()\n",
    "# df_aux_entidade"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2 prepara tabela BASE (Z_aux_BASE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-03-31T17:25:36.159978Z",
     "start_time": "2021-03-31T17:25:36.156948Z"
    }
   },
   "outputs": [],
   "source": [
    "df_aux_BASE = df_form.copy()\n",
    "df_aux_BASE.columns = ['atributo', 'nome', 'valor', 'protocolo', 'servico', 'tipo', 'campo_relacionado']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-03-31T17:25:36.946461Z",
     "start_time": "2021-03-31T17:25:36.932711Z"
    }
   },
   "outputs": [],
   "source": [
    "# transforma as strings atributo e campo relacionado em lower case\n",
    "df_aux_BASE[['atributo', 'campo_relacionado']] = df_aux_BASE[['atributo', 'campo_relacionado']].apply(lambda x: x.str.lower())\n",
    "# df_aux_BASE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-03-31T17:25:38.043281Z",
     "start_time": "2021-03-31T17:25:38.036679Z"
    }
   },
   "outputs": [],
   "source": [
    "# faz um merge da dimensão df_aux_BASE com a df_auxentidade para adicionar a coluna entidade\n",
    "# se não existir o protocolo na tasks não considera a linha -- isso é um erro de integridade da base de dados\n",
    "df_aux_BASE = df_aux_BASE.merge(df_aux_entidade, on='protocolo')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-03-31T17:25:41.256432Z",
     "start_time": "2021-03-31T17:25:41.237951Z"
    }
   },
   "outputs": [],
   "source": [
    "# cria 3 novas colunas vazias\n",
    "df_aux_BASE['MET_atributo'] = np.nan\n",
    "df_aux_BASE['MET_valor'] = np.nan\n",
    "df_aux_BASE['MET_tipo'] = np.nan\n",
    "\n",
    "# cria uma coluna como PK\n",
    "df_aux_BASE['PK'] = df_aux_BASE['entidade'] + '_' + df_aux_BASE['servico'] + '_' + \\\n",
    "                    df_aux_BASE['atributo'] + '_' + df_aux_BASE['protocolo']\n",
    "\n",
    "# cria uma coluna com o tipo de registro\n",
    "df_aux_BASE['TIPO_REG'] = 'BASE'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.3 prepara tabela MET (Z_aux_MET)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-03-31T17:25:44.616555Z",
     "start_time": "2021-03-31T17:25:44.613185Z"
    }
   },
   "outputs": [],
   "source": [
    "def Enumerico(s, tipo):\n",
    "    try:\n",
    "        v = np.float64(s) if tipo == 'DOUBLE' else np.int64(s)\n",
    "    except:\n",
    "        return False\n",
    "    \n",
    "    return True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-03-31T17:25:45.249014Z",
     "start_time": "2021-03-31T17:25:45.245371Z"
    }
   },
   "outputs": [],
   "source": [
    "def GeraCodDimMet(d, m):\n",
    "    d_dim = {'STRING_DIM': 10, 'BOOLEAN_DIM': 20, 'INTEGER_DIM' :30, 'DOUBLE_DIM': 40, 'ID_DIM': 50}\n",
    "    d_met = {'INTEGER_MET': 1, 'DOUBLE_MET': 2, 'NULL_MET': 3}\n",
    "\n",
    "    v_dim = d_dim[d] if d in d_dim else 0\n",
    "    v_met = d_met[m] if m in d_met else 0\n",
    "    \n",
    "    return v_dim + v_met"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-03-31T17:25:46.542638Z",
     "start_time": "2021-03-31T17:25:46.521820Z"
    }
   },
   "outputs": [],
   "source": [
    "# traramento do tipo INTEGER ou DOUBLE\n",
    "\n",
    "# dataset de métricas - usa somente tipo \"INTEGER\" ou \"DOUBLE\"\n",
    "cols = ['atributo', 'nome', 'valor', 'protocolo', 'servico', 'tipo', 'campo_relacionado', 'entidade']\n",
    "df_aux_MET = df_aux_BASE.loc[(df_aux_BASE['tipo'] == 'INTEGER') | (df_aux_BASE['tipo'] == 'DOUBLE'), cols].copy()\n",
    "\n",
    "# os valores estão com formato: ponto para separador de milhar e vírgula para fração\n",
    "# tira os pontos separador de milhar e substitui vírgula por ponto\n",
    "df_aux_MET['valor'] = df_aux_MET['valor'].apply(lambda x: x.replace('.', '').replace(',', '.'))\n",
    "\n",
    "# alimenta uma coluna indicadora que mostra se o valor corresponde ou não ao tipo\n",
    "df_aux_MET['numerico'] = df_aux_MET.apply(lambda x: Enumerico(x['valor'], x['tipo']), axis=1)\n",
    "\n",
    "# cria um dataset de linhas cuja o valor não corresponde ao tipo\n",
    "df_aux_MET_Err = df_aux_MET[df_aux_MET['numerico'] == False].drop('numerico', axis=1)\n",
    "\n",
    "# cria um dataset de linhas de métricas\n",
    "df_aux_MET = df_aux_MET[~df_aux_MET['numerico'] == False].drop('numerico', axis=1)\n",
    "\n",
    "# identifica que esse é um dataset de métricas\n",
    "df_aux_MET['TIPO_REG'] = 'MET'\n",
    "\n",
    "# exclui linhas que não seja uma métrica explícita\n",
    "df_aux_MET = df_aux_MET[~(df_aux_MET['campo_relacionado'] == 'null')]\n",
    "\n",
    "# cria a coluna FK para relacionar com o dataset BASE\n",
    "df_aux_MET['FK'] = df_aux_MET['entidade'] + '_' + df_aux_MET['servico'] + '_' + \\\n",
    "                   df_aux_MET['campo_relacionado'] + '_' + df_aux_MET['protocolo']\n",
    "\n",
    "# troca o nome de algumas colunas\n",
    "dc = {'atributo': 'MET_atributo', 'campo_relacionado': 'atributo', 'valor': 'MET_valor', 'tipo': 'MET_tipo'}\n",
    "df_aux_MET.rename(columns=dc, inplace = True)\n",
    "\n",
    "# faz um merge com o dataset BASE \n",
    "df_aux_MET = df_aux_MET.merge(df_aux_BASE[['PK', 'valor', 'tipo']], left_on='FK', right_on='PK')\n",
    "df_aux_MET.drop(['PK', 'FK'], axis=1, inplace=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.4 prepara tabela DIM (Z_aux_DIM)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-03-31T17:25:49.007225Z",
     "start_time": "2021-03-31T17:25:48.999855Z"
    }
   },
   "outputs": [],
   "source": [
    "# escolee as colunas que farão a cancatenação de dataframes\n",
    "cols = ['atributo', 'nome', 'valor', 'protocolo', 'servico', 'tipo', \n",
    "        'entidade', 'MET_atributo', 'MET_valor', 'MET_tipo', 'TIPO_REG']\n",
    "df_aux_DIM = df_aux_BASE[cols].copy()\n",
    "df_aux_DIM = pd.concat([df_aux_DIM, df_aux_MET])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.5 prepara tabela de decisão (Z_aux_decisao)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-03-31T17:25:50.285017Z",
     "start_time": "2021-03-31T17:25:50.276982Z"
    }
   },
   "outputs": [],
   "source": [
    "mtx =  [('STRING_DIM',  'INTEGER_MET', 'N', 'N', 'S', 'S'),\n",
    "        ('STRING_DIM',  'DOUBLE_MET',  'N', 'N', 'S', 'S'),\n",
    "        ('STRING_DIM',  'NULL_MET',    'S', 'S', 'N', 'N'),\n",
    "        \n",
    "        ('BOOLEAN_DIM', 'INTEGER_MET', 'S', 'N', 'N', 'N'),\n",
    "        ('BOOLEAN_DIM', 'DOUBLE_MET',  'S', 'N', 'N', 'N'),\n",
    "        ('BOOLEAN_DIM', 'NULL_MET',    'S', 'N', 'N', 'N'),\n",
    "        \n",
    "        ('INTEGER_DIM', 'INTEGER_MET', 'S', 'S', 'S', 'S'),\n",
    "        ('INTEGER_DIM', 'DOUBLE_MET',  'N', 'N', 'N', 'N'),\n",
    "        ('INTEGER_DIM', 'NULL_MET',    'S', 'S', 'N', 'N'),\n",
    "        \n",
    "        ('DOUBLE_MET',  'INTEGER_MET', 'N', 'N', 'N', 'N'),\n",
    "        ('DOUBLE_MET',  'DOUBLE_MET',  'N', 'N', 'S', 'S'),\n",
    "        ('DOUBLE_MET',  'NULL_MET',    'N', 'N', 'N', 'N'),\n",
    "        \n",
    "        ('ID_DIM',      'INTEGER_MET', 'N', 'N', 'S', 'S'),\n",
    "        ('ID_DIM',      'DOUBLE_MET',  'N', 'N', 'S', 'S'),\n",
    "        ('ID_DIM',      'NULL_MET',    'S', 'S', 'N', 'N')\n",
    "       ]\n",
    "cols_mtx = ['t_dim', 't_met', 'Count', 'Distinct Count', 'Sum', 'Average']\n",
    "\n",
    "df_aux_dec = pd.DataFrame(mtx, columns=cols_mtx)\n",
    "df_aux_dec['Cod_Decisao'] = df_aux_dec.apply(lambda x:\n",
    "                                    GeraCodDimMet(x['t_dim'], x['t_met']), axis=1)\n",
    "\n",
    "# df_aux_dec"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.6 ponte de decisão da operação (Z_pto_decisao_operacao)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-03-31T17:25:56.799186Z",
     "start_time": "2021-03-31T17:25:56.785990Z"
    }
   },
   "outputs": [],
   "source": [
    "# faz uma cópia da tabela de decisão com as colunas necessárias\n",
    "cols = ['Cod_Decisao'] + cols_mtx[2:]\n",
    "df_aux_pto = df_aux_dec[cols].copy()\n",
    "\n",
    "# faz o unpivot\n",
    "df_aux_pto = pd.melt(df_aux_pto, id_vars=['Cod_Decisao'], \n",
    "                 value_vars=['Count', 'Distinct Count', 'Sum', 'Average'],\n",
    "                 var_name='Atributo')\n",
    "df_aux_pto = df_aux_pto[df_aux_pto['value'] == 'S'].drop('value', axis=1)\n",
    "\n",
    "# trasnforma a coluna Atributo em uma coluna do tipo \"category'\n",
    "df_aux_pto['Atributo'] = df_aux_pto['Atributo'].astype('category')\n",
    "\n",
    "# cria uma nova coluna que será a coluna chave primária da dimensão Operação\n",
    "df_aux_pto['FK_ZN_dim_OPERACAO'] = df_aux_pto['Atributo'].cat.codes.astype('int64') + 1\n",
    "\n",
    "# cria a tabela df_pto_decisao_operacao necessária ao modelo dimensional\n",
    "df_pto_decisao_operacao = df_aux_pto[['Cod_Decisao', 'FK_ZN_dim_OPERACAO']]\n",
    "df_pto_decisao_operacao.columns = ['FK_dim_DECISAO', 'FK_ZN_dim_OPERACAO']\n",
    "# df_pto_decisao_operacao"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.7 tabela auxiliar das Datas das Solicitações (Z_aux_DataSolicitacao)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-03-31T14:33:12.061851Z",
     "start_time": "2021-03-31T14:33:12.021740Z"
    }
   },
   "outputs": [],
   "source": [
    "# cria uma coluna só de datas\n",
    "df_tasks['data_criacao'] = pd.to_datetime(df_tasks['data_hora_criacao']).dt.date\n",
    "\n",
    "# pega a menor data de cada número de protocolo\n",
    "df_aux_DataSolicitacao = df_tasks[['protocolo', 'data_criacao']].groupby('protocolo').agg('min').reset_index()\n",
    "# df_aux_DataSolicitacao"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <font color='black'>4.0 monta tabelas dimensões</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.1 monta Z_dim_DIM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-03-31T14:33:13.365161Z",
     "start_time": "2021-03-31T14:33:13.356415Z"
    }
   },
   "outputs": [],
   "source": [
    "df_dim_dim = df_aux_DIM[['atributo', 'nome']].drop_duplicates().reset_index(drop=True).reset_index()\n",
    "df_dim_dim.columns = ['PK_Z_dim_DIM', 'dimensao', 'nome']\n",
    "df_dim_dim['PK_Z_dim_DIM'] += 1\n",
    "# df_dim_dim"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.2 monta Z_dim_SERVICO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-03-31T14:33:14.710359Z",
     "start_time": "2021-03-31T14:33:14.704167Z"
    }
   },
   "outputs": [],
   "source": [
    "df_dim_servico = df_aux_DIM['servico'].drop_duplicates().reset_index(drop=True).reset_index()\n",
    "df_dim_servico.columns = ['PK_Z_dim_SERVICO', 'servico']\n",
    "df_dim_servico['PK_Z_dim_SERVICO'] += 1\n",
    "# df_dim_servico\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.3 monta Z_dim_MET"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-03-31T14:33:15.778543Z",
     "start_time": "2021-03-31T14:33:15.772095Z"
    }
   },
   "outputs": [],
   "source": [
    "df_dim_met = df_aux_DIM.loc[~df_aux_DIM['MET_atributo'].isna(), 'MET_atributo'].drop_duplicates()\n",
    "df_dim_met = df_dim_met.reset_index(drop=True).reset_index()\n",
    "df_dim_met.columns = ['PK_Z_dim_MET', 'metrica']\n",
    "df_dim_met['PK_Z_dim_MET'] += 1\n",
    "# df_dim_met\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.4 monta Z_dim_OPERACAO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-03-31T14:33:17.313808Z",
     "start_time": "2021-03-31T14:33:17.308034Z"
    }
   },
   "outputs": [],
   "source": [
    "df_ZN_dim_OPERACAO = df_aux_pto[['FK_ZN_dim_OPERACAO', 'Atributo']].drop_duplicates()\n",
    "df_ZN_dim_OPERACAO.columns = ['PK_Z_dim_OPERACAO', 'operacao']\n",
    "# df_ZN_dim_OPERACAO"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.5 monta Z_dim_DECISÃO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-03-31T14:33:18.455781Z",
     "start_time": "2021-03-31T14:33:18.451235Z"
    }
   },
   "outputs": [],
   "source": [
    "df_dim_dec = df_aux_dec[['Cod_Decisao', 't_dim', 't_met']].copy().rename(columns={'Cod_Decisao': 'PK_Z_dim_DECISAO'})\n",
    "# df_dim_dec"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.6 monta Z_fat_BETA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-03-31T14:33:21.560708Z",
     "start_time": "2021-03-31T14:33:21.360455Z"
    }
   },
   "outputs": [],
   "source": [
    "# copia o df auxiliar\n",
    "df_fat_B = df_aux_DIM.copy()\n",
    "\n",
    "# faz merge com dim_DIM\n",
    "df_fat_B = df_fat_B.merge(df_dim_dim, \n",
    "                          how='left', \n",
    "                          left_on=['atributo', 'nome'], \n",
    "                          right_on=['dimensao', 'nome']).drop(['atributo', 'nome', 'dimensao'], axis=1)\n",
    "# faz merge com dim_SERVICO\n",
    "df_fat_B = df_fat_B.merge(df_dim_servico, \n",
    "                          how='left', \n",
    "                          left_on='servico', \n",
    "                          right_on='servico').drop('servico', axis=1)\n",
    "# faz merge com dim_MET\n",
    "df_fat_B = df_fat_B.merge(df_dim_met, \n",
    "                          how='left', \n",
    "                          left_on='MET_atributo', \n",
    "                          right_on='metrica').drop(['MET_atributo', 'metrica'], axis=1)\n",
    "# faz merge com Z_aux_DataSolicitacao\n",
    "df_fat_B = df_fat_B.merge(df_aux_DataSolicitacao, \n",
    "                          how='left', \n",
    "                          on='protocolo') #.drop(['MET_atributo', 'metrica'], axis=1)\n",
    "\n",
    "# cria a coluna indicadora de NULL_MET\n",
    "df_fat_B['reg_nativo'] = df_fat_B['MET_tipo'].apply(lambda x: 1 if pd.isna(x) else 0)\n",
    "\n",
    "# passa as colunas de tipo para a string indicadora de NULL\n",
    "df_fat_B.loc[df_fat_B['MET_tipo'].isna(), 'MET_tipo'] = 'NULL_MET'\n",
    "df_fat_B.loc[df_fat_B['tipo'].isna(), 'tipo'] = 'NULL_DIM'\n",
    "\n",
    "\n",
    "# substitui valores da coluna tipo\n",
    "subs_dim = {'STRING':'STRING_DIM', \n",
    "            'INTEGER': 'INTEGER_DIM', \n",
    "            'BOOLEAN':'BOOLEAN_DIM', \n",
    "            'DOUBLE': 'DOUBLE_DIM',\n",
    "            'ID':'ID_DIM'}\n",
    "subs_met = {'INTEGER': 'INTEGER_MET', \n",
    "            'DOUBLE': 'DOUBLE_MET'}\n",
    "\n",
    "df_fat_B['tipo'].replace(subs_dim, inplace=True)\n",
    "df_fat_B['MET_tipo'].replace(subs_met, inplace=True)\n",
    "\n",
    "# acha o código para a tabela de decisão da dupla de tipos como FK para a PK de \"df_aux_dec\"\n",
    "df_fat_B['FK_dim_DECISAO'] = df_fat_B.apply(lambda x:\n",
    "                                    GeraCodDimMet(x['tipo'], x['MET_tipo']), axis=1)\n",
    "\n",
    "# exclui colunas já usadas\n",
    "df_fat_B.drop(['tipo', 'MET_tipo'], axis=1, inplace=True)\n",
    "\n",
    "# acerta os nomes das colunas\n",
    "dc = {'PK_Z_dim_DIM': 'FK_Z_dim_DIM', \n",
    "      'PK_Z_dim_SERVICO': 'FK_Z_dim_SERVICO', \n",
    "      'PK_Z_dim_MET': 'FK_Z_dim_MET', \n",
    "      'data_criacao': 'FK_Data_Criacao'}\n",
    "df_fat_B.rename(columns=dc, inplace = True)\n",
    "\n",
    "# acerta os tipos das colunas\n",
    "df_fat_B['FK_Z_dim_MET'] = df_fat_B['FK_Z_dim_MET'].astype('Int64')\n",
    "df_fat_B['MET_valor'] = df_fat_B['MET_valor'].astype('float64')\n",
    "df_fat_B['FK_Data_Criacao'] = df_fat_B['FK_Data_Criacao'].apply(pd.to_datetime)\n",
    "\n",
    "# df_fat_B.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# grava no MySQL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# monta as strings de conexão\n",
    "def StrConn(sit):\n",
    "    in_oper   = {'My_host': 'localhost', 'My_db': 'bd_fontes',       'My_user': 'gd', 'My_pw': 'Alpar@123'}\n",
    "    out_oper  = {'My_host': 'localhost', 'My_db': 'bd_dw',           'My_user': 'gd', 'My_pw': 'Alpar@123'}\n",
    "    in_teste  = {'My_host': 'localhost', 'My_db': 'bd_teste_fontes', 'My_user': 'gd', 'My_pw': 'Alpar@123'}\n",
    "    out_teste = {'My_host': 'localhost', 'My_db': 'bd_teste_dw',     'My_user': 'gd', 'My_pw': 'Alpar@123'}\n",
    "    \n",
    "    # se True é teste\n",
    "    if sit:\n",
    "        rt_in = in_teste\n",
    "        rt_out = out_teste\n",
    "    else:\n",
    "        rt_in = in_oper\n",
    "        rt_out = out_oper\n",
    "\n",
    "    return rt_in, rt_out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Load_BD(strAut):\n",
    "\n",
    "    # import the module\n",
    "    # create sqlalchemy engine\n",
    "    \n",
    "    SQLengine = create_engine(\"mysql+pymysql://{user}:{pw}@{host}/{db}\"\n",
    "                           .format(user = strAut['My_user'],\n",
    "                                   pw   = strAut['My_pw'],\n",
    "                                   host = strAut['My_host'],\n",
    "                                   db   = strAut['My_db']),\n",
    "                                   pool_recycle=3600)\n",
    "    dbConn = SQLengine.connect()\n",
    "\n",
    "    try:\n",
    "        df_dim_Acao.to_sql('dim_acoes', con=dbConn, if_exists='replace', index=False, chunksize = 1000)\n",
    "        df_dim_Categoria.to_sql('dim_categorias_servicos', con=dbConn, if_exists='replace', index=False, chunksize = 1000)\n",
    "        df_dim_Encaminhamento.to_sql('dim_encaminhamento', con=dbConn, if_exists='replace', index=False, chunksize = 1000)\n",
    "        df_dim_Entidade.to_sql('dim_entidades', con=dbConn, if_exists='replace', index=False, chunksize = 1000)\n",
    "        df_dim_GrupoResponsavel.to_sql('dim_grupo_responsavel', con=dbConn, if_exists='replace', index=False, chunksize = 1000)\n",
    "        df_dim_Grupo.to_sql('dim_grupos_usuarios', con=dbConn, if_exists='replace', index=False, chunksize = 1000)\n",
    "        df_dim_MotivoCanc.to_sql('dim_motivos_canc', con=dbConn, if_exists='replace', index=False, chunksize = 1000)\n",
    "        df_dim_Servicos.to_sql('dim_servicos', con=dbConn, if_exists='replace', index=False, chunksize = 1000)\n",
    "        df_dim_StatusExt.to_sql('dim_status_ext', con=dbConn, if_exists='replace', index=False, chunksize = 1000)\n",
    "        df_dim_Usuarios.to_sql('dim_usuarios', con=dbConn, if_exists='replace', index=False, chunksize = 1000)\n",
    "        df_dim_Situacao.to_sql('dim_situacao', con=dbConn, if_exists='replace', index=False, chunksize = 1000)\n",
    "        df_dim_SLA.to_sql('dim_sla', con=dbConn, if_exists='replace', index=False, chunksize = 1000)\n",
    "        df_dim_Endereco.to_sql('dim_endereco', con=dbConn, if_exists='replace', index=False, chunksize = 1000)\n",
    "\n",
    "        df_fat_tasks.to_sql('fat_tasks', con=dbConn, if_exists='replace', index=False, chunksize = 1000)\n",
    "\n",
    "    except ValueError as vx:\n",
    "        print('ERROR -', vx)\n",
    "\n",
    "    except Exception as ex:\n",
    "        print('EXCEPTION -', ex)\n",
    "\n",
    "    else:\n",
    "        print('Tabelas criadas com sucesso');  \n",
    "\n",
    "    finally:\n",
    "        dbConn.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TESTE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
